{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load model.py\n",
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Implements the Keras Sequential model.\"\"\"\n",
    "\n",
    "import itertools\n",
    "import multiprocessing.pool\n",
    "import threading\n",
    "from functools import partial\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from keras import layers, models\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "#from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.utils import np_utils\n",
    "from keras.backend import relu, sigmoid\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import utils\n",
    "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
    "from tensorflow.python.saved_model.signature_def_utils_impl import build_signature_def, predict_signature_def\n",
    "from tensorflow.contrib.session_bundle import exporter\n",
    "import os\n",
    "\n",
    "\n",
    "def model_fn(labels_dim):\n",
    "    \"\"\"Create a Keras Sequential model with layers.\"\"\"\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(128, 128, 3)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(labels_dim, activation='softmax'))\n",
    "\n",
    "    compile_model(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def read_train_data():\n",
    "    start_time = time.time()\n",
    "    print(\"Start Read Train Data\")\n",
    "    data = np.load(\"trainDataSmall.npz\")\n",
    "    print(\"Train data read --- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(data)\n",
    "    X_train = data[\"X_test\"] # TODO\n",
    "    Y_train = data[\"Y_test\"]\n",
    "    print(\"Training - Total examples per class\", np.sum(Y_train, axis=0))\n",
    "    return [X_train, Y_train]\n",
    "\n",
    "\n",
    "def read_test_data():\n",
    "    start_time = time.time()\n",
    "    print(\"Start Read Test Data\")\n",
    "    data = np.load(\"testDataSmall.npz\")\n",
    "    print(\"Test data read --- %s seconds ---\" % (time.time() - start_time))\n",
    "    X_test = data[\"X_test\"]\n",
    "    Y_test = data[\"Y_test\"]\n",
    "    print(\"Testing - Total examples per class\", np.sum(Y_test, axis=0))\n",
    "    return [X_test, Y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load task.py\n",
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"This code implements a Feed forward neural network using Keras API.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "import model\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "CLASS_SIZE = 5\n",
    "\n",
    "# CHUNK_SIZE specifies the number of lines\n",
    "# to read in case the file is very large\n",
    "FILE_PATH = 'checkpoint.{epoch:02d}.hdf5'\n",
    "RETINOPATHY_MODEL = 'retinopathy.hdf5'\n",
    "\n",
    "\n",
    "class ContinuousEval(keras.callbacks.Callback):\n",
    "    \"\"\"Continuous eval callback to evaluate the checkpoint once\n",
    "       every so many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 eval_frequency,\n",
    "                 job_dir):\n",
    "        self.eval_frequency = eval_frequency\n",
    "        self.job_dir = job_dir\n",
    "        [self.X_test, self.Y_test] = model.read_test_data()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if epoch > 0 and epoch % self.eval_frequency == 0:\n",
    "            # Unhappy hack to work around h5py not being able to write to GCS.\n",
    "            # Force snapshots and saves to local filesystem, then copy them over to GCS.\n",
    "            model_path_glob = 'checkpoint.*'\n",
    "            model_path_glob = os.path.join(self.job_dir, model_path_glob)\n",
    "            checkpoints = glob.glob(model_path_glob)\n",
    "            if len(checkpoints) > 0:\n",
    "                checkpoints.sort()\n",
    "                retinopathy_model = load_model(checkpoints[-1])\n",
    "                retinopathy_model = model.compile_model(retinopathy_model)\n",
    "                loss, acc = retinopathy_model.evaluate(\n",
    "                    self.X_test, self.Y_test)\n",
    "                print('\\nEvaluation epoch[{}] metrics[{:.2f}, {:.2f}] {}'.format(\n",
    "                    epoch, loss, acc, retinopathy_model.metrics_names))\n",
    "            else:\n",
    "                print('\\nEvaluation epoch[{}] (no checkpoints found)'.format(epoch))\n",
    "\n",
    "\n",
    "def dispatch(train_files,\n",
    "             eval_files,\n",
    "             job_dir,\n",
    "             train_steps,\n",
    "             eval_steps,\n",
    "             train_batch_size,\n",
    "             eval_batch_size,\n",
    "             learning_rate,\n",
    "             eval_frequency,\n",
    "             first_layer_size,\n",
    "             num_layers,\n",
    "             scale_factor,\n",
    "             eval_num_epochs,\n",
    "             num_epochs,\n",
    "             checkpoint_epochs):\n",
    "    retinopathy_model = model.model_fn(CLASS_SIZE)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(job_dir)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Unhappy hack to work around h5py not being able to write to GCS.\n",
    "    # Force snapshots and saves to local filesystem, then copy them over to GCS.\n",
    "    checkpoint_path = FILE_PATH\n",
    "    checkpoint_path = os.path.join(job_dir, checkpoint_path)\n",
    "\n",
    "    # Model checkpoint callback\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        verbose=2,\n",
    "        period=checkpoint_epochs,\n",
    "        mode='max')\n",
    "\n",
    "    # Continuous eval callback\n",
    "    evaluation = ContinuousEval(eval_frequency,\n",
    "                                job_dir)\n",
    "\n",
    "    # Tensorboard logs callback\n",
    "    tblog = keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(job_dir, 'logs'),\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        embeddings_freq=0)\n",
    "\n",
    "    callbacks = [checkpoint, evaluation, tblog]\n",
    "\n",
    "    [X_train, Y_train] = model.read_train_data()\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    retinopathy_model.fit_generator(\n",
    "        datagen.flow(X_train, Y_train, batch_size=100),\n",
    "        steps_per_epoch=100,\n",
    "        epochs=50,\n",
    "        callbacks=callbacks,\n",
    "        verbose=2,\n",
    "        validation_data=(evaluation.X_test, evaluation.Y_test))\n",
    "\n",
    "    retinopathy_model.save(os.path.join(job_dir, RETINOPATHY_MODEL))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-files',\n",
    "                        required=False,\n",
    "                        type=str,\n",
    "                        help='Training files local or GCS', nargs='+')\n",
    "    parser.add_argument('--eval-files',\n",
    "                        required=False,\n",
    "                        type=str,\n",
    "                        help='Evaluation files local or GCS', nargs='+')\n",
    "    parser.add_argument('--job-dir',\n",
    "                        required=True,\n",
    "                        type=str,\n",
    "                        help='GCS or local dir to write checkpoints and export model')\n",
    "    parser.add_argument('--train-steps',\n",
    "                        type=int,\n",
    "                        default=100,\n",
    "                        help=\"\"\"\\\n",
    "                       Maximum number of training steps to perform\n",
    "                       Training steps are in the units of training-batch-size.\n",
    "                       So if train-steps is 500 and train-batch-size if 100 then\n",
    "                       at most 500 * 100 training instances will be used to train.\n",
    "                      \"\"\")\n",
    "    parser.add_argument('--eval-steps',\n",
    "                        help='Number of steps to run evalution for at each checkpoint',\n",
    "                        default=100,\n",
    "                        type=int)\n",
    "    parser.add_argument('--train-batch-size',\n",
    "                        type=int,\n",
    "                        default=40,\n",
    "                        help='Batch size for training steps')\n",
    "    parser.add_argument('--eval-batch-size',\n",
    "                        type=int,\n",
    "                        default=40,\n",
    "                        help='Batch size for evaluation steps')\n",
    "    parser.add_argument('--learning-rate',\n",
    "                        type=float,\n",
    "                        default=0.003,\n",
    "                        help='Learning rate for SGD')\n",
    "    parser.add_argument('--eval-frequency',\n",
    "                        default=10,\n",
    "                        help='Perform one evaluation per n epochs')\n",
    "    parser.add_argument('--first-layer-size',\n",
    "                        type=int,\n",
    "                        default=256,\n",
    "                        help='Number of nodes in the first layer of DNN')\n",
    "    parser.add_argument('--num-layers',\n",
    "                        type=int,\n",
    "                        default=2,\n",
    "                        help='Number of layers in DNN')\n",
    "    parser.add_argument('--scale-factor',\n",
    "                        type=float,\n",
    "                        default=0.25,\n",
    "                        help=\"\"\"\\\n",
    "                      Rate of decay size of layer for Deep Neural Net.\n",
    "                      max(2, int(first_layer_size * scale_factor**i)) \\\n",
    "                      \"\"\")\n",
    "    parser.add_argument('--eval-num-epochs',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help='Number of epochs during evaluation')\n",
    "    parser.add_argument('--num-epochs',\n",
    "                        type=int,\n",
    "                        default=20,\n",
    "                        help='Maximum number of epochs on which to train')\n",
    "    parser.add_argument('--checkpoint-epochs',\n",
    "                        type=int,\n",
    "                        default=10,\n",
    "                        help='Checkpoint per n training epochs')\n",
    "    parse_args, unknown = parser.parse_known_args()\n",
    "\n",
    "    dispatch(**parse_args.__dict__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
